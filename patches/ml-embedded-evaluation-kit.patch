diff -r --exclude=.git --exclude=resources_downloaded --exclude=log_build_default.log -w -Naur orig/lib/ml-embedded-evaluation-kit/dependencies/core-driver/src/ethosu_common.h patched/lib/ml-embedded-evaluation-kit/dependencies/core-driver/src/ethosu_common.h
--- orig/lib/ml-embedded-evaluation-kit/dependencies/core-driver/src/ethosu_common.h	2021-10-07 14:18:59.000000000 +0100
+++ patched/lib/ml-embedded-evaluation-kit/dependencies/core-driver/src/ethosu_common.h	2021-10-07 11:36:05.000000000 +0100
@@ -104,7 +104,7 @@
 #else
 #define LOG_DEBUG(format, ...)
 #endif
-
+#define ASSERT_DISABLE 1
 #if defined(ASSERT_DISABLE)
 #define ASSERT(args)
 #else
diff -r --exclude=.git --exclude=resources_downloaded --exclude=log_build_default.log -w -Naur orig/lib/ml-embedded-evaluation-kit/dependencies/tensorflow/tensorflow/core/distributed_runtime/rpc/grpc_util.cc patched/lib/ml-embedded-evaluation-kit/dependencies/tensorflow/tensorflow/core/distributed_runtime/rpc/grpc_util.cc
--- orig/lib/ml-embedded-evaluation-kit/dependencies/tensorflow/tensorflow/core/distributed_runtime/rpc/grpc_util.cc	2021-10-07 14:19:08.000000000 +0100
+++ patched/lib/ml-embedded-evaluation-kit/dependencies/tensorflow/tensorflow/core/distributed_runtime/rpc/grpc_util.cc	2021-10-07 10:49:23.000000000 +0100
@@ -103,7 +103,7 @@
 // GrpcMaybeParseProto simply copies bytes into the string.
 bool GrpcMaybeParseProto(grpc::ByteBuffer* src, string* dst) {
   dst->clear();
-  dst->reserve(src->Length());
+  dst->reserve(src->size());
   std::vector<::grpc::Slice> slices;
   if (!src->Dump(&slices).ok()) {
     return false;
@@ -117,7 +117,7 @@
 // GrpcMaybeParseProto simply copies bytes into the tstring.
 bool GrpcMaybeParseProto(grpc::ByteBuffer* src, tstring* dst) {
   dst->clear();
-  dst->reserve(src->Length());
+  dst->reserve(src->size());
   std::vector<::grpc::Slice> slices;
   if (!src->Dump(&slices).ok()) {
     return false;
diff -r --exclude=.git --exclude=resources_downloaded --exclude=log_build_default.log -w -Naur orig/lib/ml-embedded-evaluation-kit/dependencies/tensorflow/tensorflow/lite/micro/memory_helpers.cc patched/lib/ml-embedded-evaluation-kit/dependencies/tensorflow/tensorflow/lite/micro/memory_helpers.cc
--- orig/lib/ml-embedded-evaluation-kit/dependencies/tensorflow/tensorflow/lite/micro/memory_helpers.cc	2021-10-07 14:19:18.000000000 +0100
+++ patched/lib/ml-embedded-evaluation-kit/dependencies/tensorflow/tensorflow/lite/micro/memory_helpers.cc	2021-10-07 10:48:03.000000000 +0100
@@ -100,7 +100,7 @@
   // If flatbuffer_tensor.shape == nullptr, then flatbuffer_tensor is a scalar
   // so has 1 element.
   if (flatbuffer_tensor.shape() != nullptr) {
-    for (size_t n = 0; n < flatbuffer_tensor.shape()->Length(); ++n) {
+    for (size_t n = 0; n < flatbuffer_tensor.shape()->size(); ++n) {
       element_count *= flatbuffer_tensor.shape()->Get(n);
     }
   }
diff -r --exclude=.git --exclude=resources_downloaded --exclude=log_build_default.log -w -Naur orig/lib/ml-embedded-evaluation-kit/dependencies/tensorflow/tensorflow/lite/micro/micro_allocator.cc patched/lib/ml-embedded-evaluation-kit/dependencies/tensorflow/tensorflow/lite/micro/micro_allocator.cc
--- orig/lib/ml-embedded-evaluation-kit/dependencies/tensorflow/tensorflow/lite/micro/micro_allocator.cc	2021-10-07 14:19:18.000000000 +0100
+++ patched/lib/ml-embedded-evaluation-kit/dependencies/tensorflow/tensorflow/lite/micro/micro_allocator.cc	2021-10-07 11:01:09.000000000 +0100
@@ -406,16 +406,16 @@
     // copy values from the flatbuffer into the newly allocated chunk.
     kTfLiteArrayType* array =
         reinterpret_cast<kTfLiteArrayType*>(allocator->AllocateFromTail(
-            TfLiteIntArrayGetSizeInBytes(flatbuffer_array->Length()),
+            TfLiteIntArrayGetSizeInBytes(flatbuffer_array->size()),
             alignof(kTfLiteArrayType)));
     if (array == nullptr) {
       TF_LITE_REPORT_ERROR(
           error_reporter,
           "Failed to allocate %d bytes of memory to copy an array.",
-          TfLiteIntArrayGetSizeInBytes(flatbuffer_array->Length()));
+          TfLiteIntArrayGetSizeInBytes(flatbuffer_array->size()));
       return kTfLiteError;
     }
-    array->size = flatbuffer_array->Length();
+    array->size = flatbuffer_array->size();
     for (int i = 0; i < array->size; ++i) {
       array->data[i] = flatbuffer_array->Get(i);
     }
diff -r --exclude=.git --exclude=resources_downloaded --exclude=log_build_default.log -w -Naur orig/lib/ml-embedded-evaluation-kit/dependencies/tensorflow/tensorflow/lite/micro/micro_interpreter.h patched/lib/ml-embedded-evaluation-kit/dependencies/tensorflow/tensorflow/lite/micro/micro_interpreter.h
--- orig/lib/ml-embedded-evaluation-kit/dependencies/tensorflow/tensorflow/lite/micro/micro_interpreter.h	2021-10-07 14:19:18.000000000 +0100
+++ patched/lib/ml-embedded-evaluation-kit/dependencies/tensorflow/tensorflow/lite/micro/micro_interpreter.h	2021-10-07 10:47:54.000000000 +0100
@@ -121,7 +121,7 @@
   }
 
   TfLiteTensor* input(size_t index);
-  size_t inputs_size() const { return subgraph_->inputs()->Length(); }
+  size_t inputs_size() const { return subgraph_->inputs()->size(); }
   const flatbuffers::Vector<int32_t>& inputs() const {
     return *subgraph_->inputs();
   }
@@ -137,7 +137,7 @@
   }
 
   TfLiteTensor* output(size_t index);
-  size_t outputs_size() const { return subgraph_->outputs()->Length(); }
+  size_t outputs_size() const { return subgraph_->outputs()->size(); }
   const flatbuffers::Vector<int32_t>& outputs() const {
     return *subgraph_->outputs();
   }
diff -r --exclude=.git --exclude=resources_downloaded --exclude=log_build_default.log -w -Naur orig/lib/ml-embedded-evaluation-kit/dependencies/tensorflow/tensorflow/lite/toco/graph_transformations/resolve_reduce_attributes.cc patched/lib/ml-embedded-evaluation-kit/dependencies/tensorflow/tensorflow/lite/toco/graph_transformations/resolve_reduce_attributes.cc
--- orig/lib/ml-embedded-evaluation-kit/dependencies/tensorflow/tensorflow/lite/toco/graph_transformations/resolve_reduce_attributes.cc	2021-10-07 14:19:19.000000000 +0100
+++ patched/lib/ml-embedded-evaluation-kit/dependencies/tensorflow/tensorflow/lite/toco/graph_transformations/resolve_reduce_attributes.cc	2021-10-07 10:46:20.000000000 +0100
@@ -38,7 +38,7 @@
 
   // It is ok for indices_array to have a shape for an empty tensor. In that
   // case, we don't bother setting 'axis'.
-  if (indices_array.buffer->Length() == 0) return false;
+  if (indices_array.buffer->size() == 0) return false;
 
   op->axis = indices_array.GetBuffer<ArrayDataType::kInt32>().data;
   return true;
diff -r --exclude=.git --exclude=resources_downloaded --exclude=log_build_default.log -w -Naur orig/lib/ml-embedded-evaluation-kit/dependencies/tensorflow/tensorflow/lite/toco/tflite/import.cc patched/lib/ml-embedded-evaluation-kit/dependencies/tensorflow/tensorflow/lite/toco/tflite/import.cc
--- orig/lib/ml-embedded-evaluation-kit/dependencies/tensorflow/tensorflow/lite/toco/tflite/import.cc	2021-10-07 14:19:19.000000000 +0100
+++ patched/lib/ml-embedded-evaluation-kit/dependencies/tensorflow/tensorflow/lite/toco/tflite/import.cc	2021-10-07 10:46:11.000000000 +0100
@@ -70,7 +70,7 @@
       // If the shape is 0-dimensional, make sure to record it as such,
       // as oppose to leaving the array without a shape.
       array.mutable_shape()->mutable_dims()->clear();
-      for (uint32_t i = 0; i < shape->Length(); ++i) {
+      for (uint32_t i = 0; i < shape->size(); ++i) {
         auto d = shape->Get(i);
         array.mutable_shape()->mutable_dims()->push_back(d);
       }
@@ -81,15 +81,15 @@
       // Note that tf.mini only supports a single quantization parameters for
       // the whole array.
       if (quantization->min() && quantization->max()) {
-        CHECK_EQ(1, quantization->min()->Length());
-        CHECK_EQ(1, quantization->max()->Length());
+        CHECK_EQ(1, quantization->min()->size());
+        CHECK_EQ(1, quantization->max()->size());
         MinMax& minmax = array.GetOrCreateMinMax();
         minmax.min = quantization->min()->Get(0);
         minmax.max = quantization->max()->Get(0);
       }
       if (quantization->scale() && quantization->zero_point()) {
-        CHECK_EQ(1, quantization->scale()->Length());
-        CHECK_EQ(1, quantization->zero_point()->Length());
+        CHECK_EQ(1, quantization->scale()->size());
+        CHECK_EQ(1, quantization->zero_point()->size());
         QuantizationParams& q = array.GetOrCreateQuantizationParams();
         q.scale = quantization->scale()->Get(0);
         q.zero_point = quantization->zero_point()->Get(0);
@@ -144,7 +144,7 @@
 
     // Make sure all the inputs and outputs are hooked up.
     auto inputs = input_op->inputs();
-    for (uint32_t i = 0; i < inputs->Length(); i++) {
+    for (uint32_t i = 0; i < inputs->size(); i++) {
       auto input_index = inputs->Get(i);
       // input_index == -1 indicates optional tensor.
       if (input_index != -1) {
@@ -158,7 +158,7 @@
       }
     }
     auto outputs = input_op->outputs();
-    for (int i = 0, end = outputs->Length(); i < end; i++) {
+    for (int i = 0, end = outputs->size(); i < end; i++) {
       auto output_index = outputs->Get(i);
       const std::string& output_name = tensors_table.at(output_index);
       op->outputs.push_back(output_name);
diff -r --exclude=.git --exclude=resources_downloaded --exclude=log_build_default.log -w -Naur orig/lib/ml-embedded-evaluation-kit/dependencies/tensorflow/tensorflow/lite/toco/tooling_util.cc patched/lib/ml-embedded-evaluation-kit/dependencies/tensorflow/tensorflow/lite/toco/tooling_util.cc
--- orig/lib/ml-embedded-evaluation-kit/dependencies/tensorflow/tensorflow/lite/toco/tooling_util.cc	2021-10-07 14:19:19.000000000 +0100
+++ patched/lib/ml-embedded-evaluation-kit/dependencies/tensorflow/tensorflow/lite/toco/tooling_util.cc	2021-10-07 10:46:35.000000000 +0100
@@ -1066,7 +1066,7 @@
       // Constant buffer should has a valid shape.
       CheckValidShape(array->shape());
       // The shape flat-size should agree with the buffer length.
-      CHECK_EQ(array->buffer->Length(),
+      CHECK_EQ(array->buffer->size(),
                RequiredBufferSizeForShape(array->shape()))
           << "Tensor: " << array_entry.first;
     }
@@ -1320,7 +1320,7 @@
     // to estimate its size.
     if (final_data_type != ArrayDataType::kString) {
       size_t array_byte_size =
-          lhs_array.buffer->Length() * ElementSize(final_data_type);
+          lhs_array.buffer->size() * ElementSize(final_data_type);
       if (array_byte_size < min_size) {
         // Too small; skip.
         continue;
diff -r --exclude=.git --exclude=resources_downloaded --exclude=log_build_default.log -w -Naur orig/lib/ml-embedded-evaluation-kit/dependencies/tensorflow/tensorflow/lite/tools/versioning/op_version.cc patched/lib/ml-embedded-evaluation-kit/dependencies/tensorflow/tensorflow/lite/tools/versioning/op_version.cc
--- orig/lib/ml-embedded-evaluation-kit/dependencies/tensorflow/tensorflow/lite/tools/versioning/op_version.cc	2021-10-07 14:19:19.000000000 +0100
+++ patched/lib/ml-embedded-evaluation-kit/dependencies/tensorflow/tensorflow/lite/tools/versioning/op_version.cc	2021-10-07 10:45:45.000000000 +0100
@@ -662,7 +662,7 @@
 
   // Some tests have a graph with invalid tensor index.
   TFLITE_DCHECK_GE(idx, 0);
-  if (subgraph->tensors() && idx < subgraph->tensors()->Length()) {
+  if (subgraph->tensors() && idx < subgraph->tensors()->size()) {
     return subgraph->tensors()->Get(idx)->type();
   }
   LOG(ERROR) << "Can't access tenor " << idx;
@@ -694,8 +694,8 @@
           filter_tensor->quantization();
       int num_channels = filter_tensor->shape()->Get(3);
       if (filter_quant && filter_quant->scale() &&
-          filter_quant->scale()->Length() &&
-          filter_quant->scale()->Length() == num_channels) {
+          filter_quant->scale()->size() &&
+          filter_quant->scale()->size() == num_channels) {
         op_sig.options.depthwise_conv_2d.is_per_channel_quantized = true;
       }
     } break;
@@ -727,7 +727,7 @@
     } break;
 
     case BuiltinOperator_MUL: {
-      if (op->inputs()->Length() < 2 || op->outputs()->Length() < 1) {
+      if (op->inputs()->size() < 2 || op->outputs()->size() < 1) {
         break;
       }
       const Tensor* input1_tensor =
@@ -742,10 +742,10 @@
       const QuantizationParameters* output_quant =
           output_tensor->quantization();
       if (input1_quant && input1_quant->scale() &&
-          input1_quant->scale()->Length() && input2_qunt &&
-          input2_qunt->scale() && input2_qunt->scale()->Length() &&
+          input1_quant->scale()->size() && input2_qunt &&
+          input2_qunt->scale() && input2_qunt->scale()->size() &&
           output_quant && output_quant->scale() &&
-          output_quant->scale()->Length()) {
+          output_quant->scale()->size()) {
         op_sig.options.mul.input1_scale = input1_quant->scale()->Get(0);
         op_sig.options.mul.input2_scale = input2_qunt->scale()->Get(0);
         op_sig.options.mul.output_scale = output_quant->scale()->Get(0);
@@ -805,8 +805,8 @@
           filter_tensor->quantization();
       int num_channels = filter_tensor->shape()->Get(0);
       if (filter_quant && filter_quant->scale() &&
-          filter_quant->scale()->Length() &&
-          filter_quant->scale()->Length() == num_channels) {
+          filter_quant->scale()->size() &&
+          filter_quant->scale()->size() == num_channels) {
         op_sig.options.conv_2d.is_per_channel_quantized = true;
       }
     } break;
@@ -838,11 +838,11 @@
       break;
   }
 
-  for (int32_t i = 0; i < op->inputs()->Length(); ++i) {
+  for (int32_t i = 0; i < op->inputs()->size(); ++i) {
     TensorType tensor_type = GetTensorType(op->inputs()->Get(i), subgraph);
     op_sig.input_types.push_back(tensor_type);
   }
-  for (int32_t i = 0; i < op->outputs()->Length(); ++i) {
+  for (int32_t i = 0; i < op->outputs()->size(); ++i) {
     TensorType tensor_type = GetTensorType(op->outputs()->Get(i), subgraph);
     op_sig.output_types.push_back(tensor_type);
   }
@@ -853,9 +853,9 @@
   auto model = GetMutableModel(model_buffer_pointer);
   auto subgraphs = model->subgraphs();
 
-  for (int i = 0; i < subgraphs->Length(); ++i) {
+  for (int i = 0; i < subgraphs->size(); ++i) {
     const SubGraph* subgraph = subgraphs->Get(i);
-    for (int j = 0; j < subgraph->operators()->Length(); ++j) {
+    for (int j = 0; j < subgraph->operators()->size(); ++j) {
       const Operator* op = subgraph->operators()->Get(j);
       OperatorCode* op_code =
           model->mutable_operator_codes()->GetMutableObject(op->opcode_index());
diff -r --exclude=.git --exclude=resources_downloaded --exclude=log_build_default.log -w -Naur orig/lib/ml-embedded-evaluation-kit/dependencies/tensorflow/tensorflow/lite/tools/versioning/runtime_version.cc patched/lib/ml-embedded-evaluation-kit/dependencies/tensorflow/tensorflow/lite/tools/versioning/runtime_version.cc
--- orig/lib/ml-embedded-evaluation-kit/dependencies/tensorflow/tensorflow/lite/tools/versioning/runtime_version.cc	2021-10-07 14:19:19.000000000 +0100
+++ patched/lib/ml-embedded-evaluation-kit/dependencies/tensorflow/tensorflow/lite/tools/versioning/runtime_version.cc	2021-10-07 10:16:36.000000000 +0100
@@ -364,9 +364,9 @@
   auto model = GetMutableModel(model_buffer_pointer);
   std::string model_min_version;
   auto subgraphs = model->subgraphs();
-  for (int i = 0; i < subgraphs->Length(); ++i) {
+  for (int i = 0; i < subgraphs->size(); ++i) {
     const SubGraph* subgraph = subgraphs->Get(i);
-    for (int j = 0; j < subgraph->operators()->Length(); ++j) {
+    for (int j = 0; j < subgraph->operators()->size(); ++j) {
       const Operator* op = subgraph->operators()->Get(j);
       const OperatorCode* op_code =
           model->operator_codes()->Get(op->opcode_index());
diff -r --exclude=.git --exclude=resources_downloaded --exclude=log_build_default.log -w -Naur orig/lib/ml-embedded-evaluation-kit/source/application/hal/platforms/bare-metal/bsp/bsp-core/retarget.c patched/lib/ml-embedded-evaluation-kit/source/application/hal/platforms/bare-metal/bsp/bsp-core/retarget.c
--- orig/lib/ml-embedded-evaluation-kit/source/application/hal/platforms/bare-metal/bsp/bsp-core/retarget.c	2021-10-07 13:17:06.000000000 +0100
+++ patched/lib/ml-embedded-evaluation-kit/source/application/hal/platforms/bare-metal/bsp/bsp-core/retarget.c	2021-10-07 12:43:38.000000000 +0100
@@ -197,7 +197,8 @@
 
 void RETARGET(_exit)(int return_code)
 {
-    UartEndSimulation(return_code);
+    // TODO: unify drivers in SDK
+    //UartEndSimulation(return_code);
 }
 
 int system(const char *cmd)
@@ -241,18 +242,18 @@
     return 0;
 }
 
-int fputc(int ch, FILE *f)
+// Commented out in favor of the tfm implementation
+/*int fputc(int ch, FILE *f)
 {
     UNUSED(f);
-
     return UartPutc(ch);
-}
+}*/
 
 int fgetc(FILE *f)
 {
     UNUSED(f);
-
-    return UartPutc(UartGetc());
+    // TODO: Unify drivers in SDK
+    return 0;//UartPutc(UartGetc());
 }
 
 #ifndef ferror
diff -r --exclude=.git --exclude=resources_downloaded --exclude=log_build_default.log -w -Naur orig/lib/ml-embedded-evaluation-kit/source/application/hal/platforms/bare-metal/bsp/bsp-packs/mps3/include/smm_mps3.h patched/lib/ml-embedded-evaluation-kit/source/application/hal/platforms/bare-metal/bsp/bsp-packs/mps3/include/smm_mps3.h
--- orig/lib/ml-embedded-evaluation-kit/source/application/hal/platforms/bare-metal/bsp/bsp-packs/mps3/include/smm_mps3.h	2021-10-07 13:17:06.000000000 +0100
+++ patched/lib/ml-embedded-evaluation-kit/source/application/hal/platforms/bare-metal/bsp/bsp-packs/mps3/include/smm_mps3.h	2021-10-07 12:37:20.000000000 +0100
@@ -18,7 +18,6 @@
 #define SMM_MPS3_H
 
 #include "cmsis.h"                  /* Device specific header file. */
-#include "peripheral_memmap.h"      /* Peripheral memory map definitions. */
 
 #if defined ( __CC_ARM   )
 #pragma anon_unions
